{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bb93c6f",
   "metadata": {
    "papermill": {
     "duration": 0.00747,
     "end_time": "2023-11-08T03:38:31.374327",
     "exception": false,
     "start_time": "2023-11-08T03:38:31.366857",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Citations\n",
    "#### Dataset\n",
    "“Stratosphere Laboratory. A labeled dataset with malicious and benign IoT network traffic. January 22th. Agustin Parmisano, Sebastian Garcia, Maria Jose Erquiaga. https://www.stratosphereips.org/datasets-iot23\n",
    "#### Model advices\n",
    "Christian Desrosiers, École de Technologie Supérieur (ETS), for proposing XGBoost model.\n",
    "#### Notebook creation\n",
    "Rémi Blier, École de Technologie Supérieur (ETS), for creating this notebook.\n",
    "#### XGBoost model\n",
    "Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 785–794). New York, NY, USA: ACM. https://doi.org/10.1145/2939672.2939785"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8edd75",
   "metadata": {
    "papermill": {
     "duration": 0.004499,
     "end_time": "2023-11-08T03:38:31.384718",
     "exception": false,
     "start_time": "2023-11-08T03:38:31.380219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "726e213e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T03:38:31.397030Z",
     "iopub.status.busy": "2023-11-08T03:38:31.396342Z",
     "iopub.status.idle": "2023-11-08T03:39:04.242335Z",
     "shell.execute_reply": "2023-11-08T03:39:04.240846Z"
    },
    "papermill": {
     "duration": 32.857328,
     "end_time": "2023-11-08T03:39:04.247633",
     "exception": false,
     "start_time": "2023-11-08T03:38:31.390305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/network-malware-detection-connection-analysis/CTU-IoT-Malware-Capture-60-1conn.log.labeled.csv\n",
      "/kaggle/input/network-malware-detection-connection-analysis/CTU-IoT-Malware-Capture-21-1conn.log.labeled.csv\n",
      "(3584314, 23)\n"
     ]
    }
   ],
   "source": [
    "def find_csv_delimiter(file_path, max_lines=5):\n",
    "    with open(file_path, 'r', newline='') as file:\n",
    "        sample_lines = [file.readline().strip() for _ in range(max_lines)]\n",
    "\n",
    "    delimiters = [',', ';', '\\t', '|']  # Common delimiters to check\n",
    "\n",
    "    best_delimiter = ','\n",
    "    max_delimiter_count = 0\n",
    "\n",
    "    for delimiter in delimiters:\n",
    "        delimiter_count = sum(line.count(delimiter) for line in sample_lines)\n",
    "        if delimiter_count > max_delimiter_count:\n",
    "            best_delimiter = delimiter\n",
    "            max_delimiter_count = delimiter_count\n",
    "\n",
    "    return best_delimiter\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def extract_data_from_csv(file_path, delimiter=','):\n",
    "    data = []  # Create a list to store the data\n",
    "\n",
    "    # Open the CSV file for reading\n",
    "    with open(file_path, mode='r', newline='') as file:\n",
    "        # Create a CSV reader object with the pipe delimiter\n",
    "\n",
    "        csv_reader = csv.reader(file, delimiter=delimiter)\n",
    "\n",
    "        # Read the header row\n",
    "        header = next(csv_reader)\n",
    "\n",
    "        # Iterate through the rows in the CSV file\n",
    "        for row in csv_reader:\n",
    "            data.append(row)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = []\n",
    "\n",
    "import os\n",
    "cpt = 0\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    \n",
    "        for filename in filenames:\n",
    "            if cpt < 2:\n",
    "                file_path = os.path.join(dirname, filename)\n",
    "                delimiter = find_csv_delimiter(file_path)\n",
    "                data += extract_data_from_csv(file_path, delimiter)\n",
    "                print(os.path.join(dirname, filename))\n",
    "                cpt+=1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "\n",
    "# Convert your data to a NumPy array\n",
    "data = np.array(data)\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8256dd68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T03:39:04.260091Z",
     "iopub.status.busy": "2023-11-08T03:39:04.259582Z",
     "iopub.status.idle": "2023-11-08T03:39:04.265517Z",
     "shell.execute_reply": "2023-11-08T03:39:04.264169Z"
    },
    "papermill": {
     "duration": 0.015579,
     "end_time": "2023-11-08T03:39:04.268499",
     "exception": false,
     "start_time": "2023-11-08T03:39:04.252920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1568940058.775322' 'CEjjLs1iwWYrt8C4O6' '192.168.1.195' '123'\n",
      " '212.111.30.190' '123' 'udp' '-' '0.008239' '96' '96' 'SF' '-' '-' '0'\n",
      " 'Dd' '2' '152' '2' '152' '-' 'Benign' '-']\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0fd7f4",
   "metadata": {
    "papermill": {
     "duration": 0.005343,
     "end_time": "2023-11-08T03:39:04.279873",
     "exception": false,
     "start_time": "2023-11-08T03:39:04.274530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "376404bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T03:39:04.292865Z",
     "iopub.status.busy": "2023-11-08T03:39:04.292493Z",
     "iopub.status.idle": "2023-11-08T03:39:50.109522Z",
     "shell.execute_reply": "2023-11-08T03:39:50.107988Z"
    },
    "papermill": {
     "duration": 45.827004,
     "end_time": "2023-11-08T03:39:50.112339",
     "exception": false,
     "start_time": "2023-11-08T03:39:04.285335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_to_remove = [0, 1, 2, 4, 12, 13, 14, 20, 22]\n",
    "\n",
    "# Remove columns using NumPy's array slicing\n",
    "data = np.delete(data, columns_to_remove, axis=1)\n",
    "\n",
    "# Make Malicious = 1 and Benign = 0\n",
    "for row in data:\n",
    "    if row[-1] == 'Benign':\n",
    "        row[-1] = 0\n",
    "    else:\n",
    "        row[-1] = 1\n",
    "\n",
    "columns_to_transform = [3, 4, 5, 6, 8]\n",
    "# Columns to put 0 if '-'\n",
    "zeros = [4, 5, 6]\n",
    "# Replace '-'\n",
    "for row in data:\n",
    "    for column in columns_to_transform:\n",
    "        if row[column] == '-' and column in zeros:\n",
    "            row[column] = 0\n",
    "        elif row[column] == '-' and column not in zeros:\n",
    "            row[column] = 'Unkown'\n",
    "\n",
    "columns_to_convert_to_float = [4]\n",
    "# Convert columns to float\n",
    "for row in data:\n",
    "    for column in columns_to_convert_to_float:\n",
    "        row[column] = float(row[column])\n",
    "\n",
    "columns_to_convert_to_int = [0, 1, 5, 6, 8, 9, 10, 11]\n",
    "\n",
    "# Convert columns to int\n",
    "for row in data:\n",
    "    for column in columns_to_convert_to_int:\n",
    "        try:\n",
    "            # Attempt to convert the value to an integer\n",
    "            row[column] = int(row[column])\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "\n",
    "# Remove rows where first column contains ip address\n",
    "rows_to_remove = []\n",
    "for index, row in enumerate(data):\n",
    "    # If the first column is an IP address\n",
    "    if row[0].count('.') == 3:\n",
    "        rows_to_remove.append(index)\n",
    "\n",
    "# Delete the rows by index\n",
    "for index in sorted(rows_to_remove, reverse=True):\n",
    "    del data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ae8e86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T03:39:50.124868Z",
     "iopub.status.busy": "2023-11-08T03:39:50.124417Z",
     "iopub.status.idle": "2023-11-08T03:41:57.487308Z",
     "shell.execute_reply": "2023-11-08T03:41:57.485856Z"
    },
    "papermill": {
     "duration": 127.376677,
     "end_time": "2023-11-08T03:41:57.494450",
     "exception": false,
     "start_time": "2023-11-08T03:39:50.117773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before one-hot encoding features:\n",
      "['123' '123' 'udp' 'Unkown' '0.008239' '96' '96' 'SF' 'Dd' '2' '152' '2'\n",
      " '152' '0']\n",
      "(14,)\n",
      "After one-hot encoding features:\n",
      "['123' '123' '0.0' '0.0' '1.0' '1.0' '0.0' '0.0' '0.008239' '96' '96'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '1.0' '0.0' '0.0' '1.0' '0.0' '0.0'\n",
      " '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '0.0' '2' '152' '2' '152'\n",
      " '0']\n",
      "(37,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# columns_to_onehot = [0, 1, 2, 3, 7, 8]\n",
    "columns_to_onehot = [2, 3, 7, 8]\n",
    "\n",
    "print('Before one-hot encoding features:')\n",
    "print(data[0])\n",
    "print(data[0].shape)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse_output=True)\n",
    "\n",
    "dataCopy = data.copy()\n",
    "\n",
    "addedCols = 0\n",
    "for column in columns_to_onehot:\n",
    "    column_values = data[:, column]\n",
    "    onehot_encoded = onehot_encoder.fit_transform(column_values.reshape(-1, 1)).toarray()\n",
    "    dataCopy = np.delete(dataCopy, column + addedCols, axis=1)\n",
    "    \n",
    "    # Insert the new columns\n",
    "    for i, encoded_column in enumerate(onehot_encoded.T):\n",
    "        dataCopy = np.insert(dataCopy, column + i + addedCols, encoded_column, axis=1)\n",
    "\n",
    "    addedCols += onehot_encoded.shape[1] - 1\n",
    "\n",
    "data = dataCopy\n",
    "\n",
    "\n",
    "print('After one-hot encoding features:')\n",
    "print(data[0])\n",
    "print(data[0].shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69bf4e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T03:41:57.505903Z",
     "iopub.status.busy": "2023-11-08T03:41:57.505540Z",
     "iopub.status.idle": "2023-11-08T03:44:50.995724Z",
     "shell.execute_reply": "2023-11-08T03:44:50.994005Z"
    },
    "papermill": {
     "duration": 173.500798,
     "end_time": "2023-11-08T03:44:51.000215",
     "exception": false,
     "start_time": "2023-11-08T03:41:57.499417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if any of the data contains strings\n",
    "for row in data:\n",
    "    for column in row:\n",
    "        if isinstance(column, str):\n",
    "            #Convert the value to a float, if possible\n",
    "            try:\n",
    "                column = float(column)\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "# Initialize an empty list to store preprocessed data\n",
    "preprocessed_data = []\n",
    "\n",
    "# Iterate through the rows in the data\n",
    "for row in data:\n",
    "    try:\n",
    "        # Convert all columns to floats in this row\n",
    "        float_row = [float(column) if column != '-' else 0.0 for column in row]\n",
    "        preprocessed_data.append(float_row)\n",
    "    except ValueError:\n",
    "        print('Skipping row with non-convertible values:', row)\n",
    "\n",
    "data = preprocessed_data\n",
    "\n",
    "# Check if data contains strings\n",
    "for row in data:\n",
    "    for column in row:\n",
    "        if isinstance(column, str):\n",
    "            print('Error: String found in data: ', column)\n",
    "            break\n",
    "\n",
    "preprocessed_data = data\n",
    "\n",
    "# Convert preprocessed_data to a normal Python list of lists\n",
    "preprocessed_data = [list(row) for row in preprocessed_data]\n",
    "\n",
    "# # Print the preprocessed data\n",
    "# for row in preprocessed_data:\n",
    "#     print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2826ef",
   "metadata": {
    "papermill": {
     "duration": 0.005067,
     "end_time": "2023-11-08T03:44:51.012309",
     "exception": false,
     "start_time": "2023-11-08T03:44:51.007242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Separate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c96722f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T03:44:51.026266Z",
     "iopub.status.busy": "2023-11-08T03:44:51.025856Z",
     "iopub.status.idle": "2023-11-08T03:44:54.434661Z",
     "shell.execute_reply": "2023-11-08T03:44:54.432911Z"
    },
    "papermill": {
     "duration": 3.420089,
     "end_time": "2023-11-08T03:44:54.437697",
     "exception": false,
     "start_time": "2023-11-08T03:44:51.017608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Separate data\n",
    "\n",
    "# Define the split ratios for training, validation, and test datasets\n",
    "train_ratio = 0.70  # 70% for training\n",
    "val_ratio = 0.15   # 15% for validation\n",
    "test_ratio = 0.15  # 15% for testing\n",
    "\n",
    "train_val_indices = int((train_ratio + val_ratio) * len(preprocessed_data))\n",
    "\n",
    "train_val_data = preprocessed_data[:train_val_indices]\n",
    "test_data = preprocessed_data[train_val_indices:]\n",
    "\n",
    "# Shuffle the data randomly\n",
    "random.shuffle(train_val_data)\n",
    "random.shuffle(test_data)\n",
    "\n",
    "# Calculate the split points\n",
    "total_records = len(train_val_data)\n",
    "train_split = int(train_ratio * total_records)\n",
    "val_split = int(val_ratio * total_records)\n",
    "\n",
    "# Split the data into training, validation\n",
    "train_data = train_val_data[:train_split]\n",
    "val_data = train_val_data[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4f121ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T03:44:54.452699Z",
     "iopub.status.busy": "2023-11-08T03:44:54.452228Z",
     "iopub.status.idle": "2023-11-08T03:45:18.113446Z",
     "shell.execute_reply": "2023-11-08T03:45:18.111886Z"
    },
    "papermill": {
     "duration": 23.671567,
     "end_time": "2023-11-08T03:45:18.116251",
     "exception": false,
     "start_time": "2023-11-08T03:44:54.444684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "val_labels = []\n",
    "test_labels = []\n",
    "\n",
    "# Separate the labels from features vectors\n",
    "train_temp = []\n",
    "for row in train_data:\n",
    "    newRow = []\n",
    "    newRow = row[:len(row)-1]\n",
    "    train_temp.append(newRow)\n",
    "    train_labels.append(row[-1])\n",
    "    \n",
    "val_temp = []\n",
    "for row in val_data:\n",
    "    newRow = []\n",
    "    newRow = row[:len(row)-1]\n",
    "    val_temp.append(newRow)\n",
    "    val_labels.append(row[-1])\n",
    "\n",
    "test_temp = []\n",
    "for row in test_data:\n",
    "    newRow = []\n",
    "    newRow = row[:len(row)-1]\n",
    "    test_temp.append(newRow)\n",
    "    test_labels.append(row[-1])\n",
    "\n",
    "train_data = train_temp\n",
    "val_data = val_temp\n",
    "test_data = test_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "120f485c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T03:45:18.128636Z",
     "iopub.status.busy": "2023-11-08T03:45:18.128195Z",
     "iopub.status.idle": "2023-11-08T03:45:31.777966Z",
     "shell.execute_reply": "2023-11-08T03:45:31.776471Z"
    },
    "papermill": {
     "duration": 13.658628,
     "end_time": "2023-11-08T03:45:31.780257",
     "exception": false,
     "start_time": "2023-11-08T03:45:18.121629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2132666, 36)\n",
      "(2132666,)\n",
      "(914000, 36)\n",
      "(914000,)\n"
     ]
    }
   ],
   "source": [
    "# Convert your data to NumPy arrays\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "val_data = np.array(val_data)\n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Print rows and columns of the data\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(val_data.shape)\n",
    "print(val_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1402d38",
   "metadata": {
    "papermill": {
     "duration": 0.004808,
     "end_time": "2023-11-08T03:45:31.790437",
     "exception": false,
     "start_time": "2023-11-08T03:45:31.785629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bd49e2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T03:45:31.806318Z",
     "iopub.status.busy": "2023-11-08T03:45:31.804728Z",
     "iopub.status.idle": "2023-11-08T03:46:05.366043Z",
     "shell.execute_reply": "2023-11-08T03:46:05.364790Z"
    },
    "papermill": {
     "duration": 33.571717,
     "end_time": "2023-11-08T03:46:05.368574",
     "exception": false,
     "start_time": "2023-11-08T03:45:31.796857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras_tuner.tuners import BayesianOptimization\n",
    "\n",
    "# Check if train data contains strings\n",
    "for row in train_data:\n",
    "    for column in row:\n",
    "        if isinstance(column, str):\n",
    "            print('Error: String found in data: ', column)\n",
    "            break\n",
    "\n",
    "# Check if val data contains strings\n",
    "for row in val_data:\n",
    "    for column in row:\n",
    "        if isinstance(column, str):\n",
    "            print('Error: String found in data: ', column)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a46b7e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T03:46:05.381727Z",
     "iopub.status.busy": "2023-11-08T03:46:05.381037Z",
     "iopub.status.idle": "2023-11-08T03:46:13.027550Z",
     "shell.execute_reply": "2023-11-08T03:46:13.026020Z"
    },
    "papermill": {
     "duration": 7.657142,
     "end_time": "2023-11-08T03:46:13.030877",
     "exception": false,
     "start_time": "2023-11-08T03:46:05.373735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52d67ddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T03:46:13.044669Z",
     "iopub.status.busy": "2023-11-08T03:46:13.044257Z",
     "iopub.status.idle": "2023-11-08T04:18:42.270707Z",
     "shell.execute_reply": "2023-11-08T04:18:42.268366Z"
    },
    "papermill": {
     "duration": 1949.2394,
     "end_time": "2023-11-08T04:18:42.276699",
     "exception": false,
     "start_time": "2023-11-08T03:46:13.037299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Define your XGBoost classifier and hyperparameter search space\n",
    "xgb_model = XGBClassifier()\n",
    "param_space = {\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "# Create a RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_space,\n",
    "    n_iter=5,  # Adjust the number of iterations as needed\n",
    "    scoring='accuracy',  # Use the appropriate scoring metric\n",
    "    n_jobs=-1,  # Use all available CPU cores for parallel processing\n",
    "    cv=5,  # Number of cross-validation folds\n",
    "    random_state=42,  # Set a random seed for reproducibility\n",
    "    verbose=3\n",
    ")\n",
    "\n",
    "# Perform hyperparameter optimization\n",
    "random_search.fit(train_data, train_labels)\n",
    "\n",
    "# Get the best hyperparameters and the best model\n",
    "best_xgb_hps = random_search.best_params_\n",
    "best_xgb_model = random_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "781819f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T04:18:42.292116Z",
     "iopub.status.busy": "2023-11-08T04:18:42.291617Z",
     "iopub.status.idle": "2023-11-08T04:18:42.300461Z",
     "shell.execute_reply": "2023-11-08T04:18:42.297635Z"
    },
    "papermill": {
     "duration": 0.020868,
     "end_time": "2023-11-08T04:18:42.304420",
     "exception": false,
     "start_time": "2023-11-08T04:18:42.283552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.8, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 0.2, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "print(best_xgb_hps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c18d0",
   "metadata": {
    "papermill": {
     "duration": 0.005798,
     "end_time": "2023-11-08T04:18:42.317606",
     "exception": false,
     "start_time": "2023-11-08T04:18:42.311808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Evaluate model on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6cb72bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T04:18:42.332142Z",
     "iopub.status.busy": "2023-11-08T04:18:42.331194Z",
     "iopub.status.idle": "2023-11-08T04:18:44.615992Z",
     "shell.execute_reply": "2023-11-08T04:18:44.614948Z"
    },
    "papermill": {
     "duration": 2.295544,
     "end_time": "2023-11-08T04:18:44.619567",
     "exception": false,
     "start_time": "2023-11-08T04:18:42.324023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy (Random Forest): 0.9999956236323851\n",
      "Validation Classification Report (XGBoost):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      1.00       623\n",
      "         1.0       1.00      1.00      1.00    913377\n",
      "\n",
      "    accuracy                           1.00    914000\n",
      "   macro avg       1.00      1.00      1.00    914000\n",
      "weighted avg       1.00      1.00      1.00    914000\n",
      "\n",
      "Validation Confusion Matrix (XGBoost):\n",
      "[[   619      4]\n",
      " [     0 913377]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Evaluate the Random Forest model on the validation data\n",
    "sgboost_val_predictions = random_search.predict(val_data)\n",
    "sgboost_val_accuracy = np.mean(sgboost_val_predictions == val_labels)\n",
    "print(\"Validation Accuracy (Random Forest):\", sgboost_val_accuracy)\n",
    "\n",
    "# Calculate and print classification report and confusion matrix for Random Forest\n",
    "sgboost_val_report = classification_report(val_labels, sgboost_val_predictions)\n",
    "sgboost_val_confusion = confusion_matrix(val_labels, sgboost_val_predictions)\n",
    "print(\"Validation Classification Report (XGBoost):\")\n",
    "print(sgboost_val_report)\n",
    "print(\"Validation Confusion Matrix (XGBoost):\")\n",
    "print(sgboost_val_confusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d98913",
   "metadata": {
    "papermill": {
     "duration": 0.006387,
     "end_time": "2023-11-08T04:18:44.632227",
     "exception": false,
     "start_time": "2023-11-08T04:18:44.625840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Evaluate model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77be544c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T04:18:44.646316Z",
     "iopub.status.busy": "2023-11-08T04:18:44.645907Z",
     "iopub.status.idle": "2023-11-08T04:18:45.929027Z",
     "shell.execute_reply": "2023-11-08T04:18:45.927544Z"
    },
    "papermill": {
     "duration": 1.294167,
     "end_time": "2023-11-08T04:18:45.932453",
     "exception": false,
     "start_time": "2023-11-08T04:18:44.638286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (SGBoost): 0.9999721007052942\n",
      "Test Classification Report (XGBoost):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      3601\n",
      "         1.0       1.00      1.00      1.00    534047\n",
      "\n",
      "    accuracy                           1.00    537648\n",
      "   macro avg       1.00      1.00      1.00    537648\n",
      "weighted avg       1.00      1.00      1.00    537648\n",
      "\n",
      "Test Confusion Matrix (XGBoost):\n",
      "[[  3597      4]\n",
      " [    11 534036]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Random Forest model on the test data\n",
    "sgboost_test_predictions = random_search.predict(test_data)\n",
    "sgboost_test_accuracy = np.mean(sgboost_test_predictions == test_labels)\n",
    "print(\"Test Accuracy (SGBoost):\", sgboost_test_accuracy)\n",
    "\n",
    "# Calculate and print classification report and confusion matrix for Random Forest\n",
    "sgboost_test_report = classification_report(test_labels, sgboost_test_predictions)\n",
    "sgboost_test_confusion = confusion_matrix(test_labels, sgboost_test_predictions)\n",
    "print(\"Test Classification Report (XGBoost):\")\n",
    "print(sgboost_test_report)\n",
    "print(\"Test Confusion Matrix (XGBoost):\")\n",
    "print(sgboost_test_confusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2421.633294,
   "end_time": "2023-11-08T04:18:49.709107",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-08T03:38:28.075813",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
